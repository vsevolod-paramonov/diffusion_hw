{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6e4ca8ea-74b8-424f-b349-7b89a651a0a9",
      "metadata": {
        "id": "6e4ca8ea-74b8-424f-b349-7b89a651a0a9"
      },
      "source": [
        "# 3. Использование диффузионных моделей для непарного переноса стиля"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e8bfd3-b7aa-4d96-a4c3-b00dbba672fe",
      "metadata": {
        "id": "08e8bfd3-b7aa-4d96-a4c3-b00dbba672fe"
      },
      "source": [
        "На 5 лекции мы имплементировали диффузионные модели для безусловной генерации и генерации при условии метки класса. В первой части домашки мы более глубоко исследовали генерацию при условии метки класса, а во второй — условную генерацию для решения парных (в т.ч. обратных) задач. Наконец, в третьей части мы разберем два простых метода, которые позволяют применять диффузионные модели для решения непарных задач перевода между доменами (непарного переноса стиля)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84cf8a9f-b6af-4ed0-9be2-75d4223cbd12",
      "metadata": {
        "id": "84cf8a9f-b6af-4ed0-9be2-75d4223cbd12"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c651c37a-752f-4c02-9a51-6b26a28496a9",
      "metadata": {
        "id": "c651c37a-752f-4c02-9a51-6b26a28496a9"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import Compose, Resize, ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b06e836-7669-40fc-97be-794f8d2f0421",
      "metadata": {
        "id": "5b06e836-7669-40fc-97be-794f8d2f0421"
      },
      "source": [
        "## Цветной MNIST\n",
        "\n",
        "В домашке предлагается поработать с цветной модификацией датасета MNIST (код для покраски взят [у коллег](https://github.com/ngushchin/EntropicNeuralOptimalTransport/blob/main/src/tools.py) из Сколтеха). С одной стороны, такой датасет все еще оставляет возможность обучать диффузионные модели, но делает свойства модели более интерпретируемыми (например, в задачах условной генерации, таких, как дорисовывание, повышение разрешения и деблюринг, можно отследить корректное сохранение цвета изображения)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bdbf53c-451b-45fd-8f43-0203d1c47ce1",
      "metadata": {
        "id": "1bdbf53c-451b-45fd-8f43-0203d1c47ce1"
      },
      "outputs": [],
      "source": [
        "class ColoredMNIST(MNIST):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hues = 360 * torch.rand(super().__len__())\n",
        "\n",
        "    def __len__(self):\n",
        "        return super().__len__()\n",
        "\n",
        "    def color_image(self, img, idx):\n",
        "        img_min = 0\n",
        "        a = (img - img_min) * (self.hues[idx] % 60) / 60\n",
        "        img_inc = a\n",
        "        img_dec = img - a\n",
        "\n",
        "        colored_image = torch.zeros((3, img.shape[1], img.shape[2]))\n",
        "        H_i = round(self.hues[idx].item() / 60) % 6\n",
        "\n",
        "        if H_i == 0:\n",
        "            colored_image[0] = img\n",
        "            colored_image[1] = img_inc\n",
        "            colored_image[2] = img_min\n",
        "        elif H_i == 1:\n",
        "            colored_image[0] = img_dec\n",
        "            colored_image[1] = img\n",
        "            colored_image[2] = img_min\n",
        "        elif H_i == 2:\n",
        "            colored_image[0] = img_min\n",
        "            colored_image[1] = img\n",
        "            colored_image[2] = img_inc\n",
        "        elif H_i == 3:\n",
        "            colored_image[0] = img_min\n",
        "            colored_image[1] = img_dec\n",
        "            colored_image[2] = img\n",
        "        elif H_i == 4:\n",
        "            colored_image[0] = img_inc\n",
        "            colored_image[1] = img_min\n",
        "            colored_image[2] = img\n",
        "        elif H_i == 5:\n",
        "            colored_image[0] = img\n",
        "            colored_image[1] = img_min\n",
        "            colored_image[2] = img_dec\n",
        "\n",
        "        return colored_image\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = super().__getitem__(idx)\n",
        "        return self.color_image(img, idx), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d429c3cb-fca5-4f9f-9c65-823342e981a6",
      "metadata": {
        "id": "d429c3cb-fca5-4f9f-9c65-823342e981a6"
      },
      "outputs": [],
      "source": [
        "transform = Compose([Resize((32, 32)), ToTensor()])\n",
        "data_train = ColoredMNIST(root='.', train=True, download=False, transform=transform)\n",
        "#data_train = ColoredMNIST(root='.', train=True, download=True, transform=transform)\n",
        "data_test = ColoredMNIST(root='.', train=False, download=False, transform=transform)\n",
        "train_dataloader = DataLoader(data_train, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(data_test, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f224c9-016a-4cdd-9bef-45ee4febc2be",
      "metadata": {
        "id": "10f224c9-016a-4cdd-9bef-45ee4febc2be"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "def remove_ticks(ax):\n",
        "    ax.tick_params(\n",
        "        axis='both',\n",
        "        which='both',\n",
        "        bottom=False,\n",
        "        top=False,\n",
        "        labelbottom=False,\n",
        "        left=False,\n",
        "        labelleft=False\n",
        "    )\n",
        "\n",
        "def remove_xticks(ax):\n",
        "    ax.tick_params(\n",
        "        axis='both',\n",
        "        which='both',\n",
        "        bottom=False,\n",
        "        top=False,\n",
        "        labelbottom=False,\n",
        "        left=True,\n",
        "        labelleft=True\n",
        "    )\n",
        "\n",
        "def visualize_batch(img_vis, title='Семплы из цветного MNIST', nrow=10, ncol=4):\n",
        "    img_grid = make_grid(img_vis, nrow=nrow)\n",
        "    fig, ax = plt.subplots(1, figsize=(nrow, ncol))\n",
        "    remove_ticks(ax)\n",
        "    ax.set_title(title, fontsize=14)\n",
        "    ax.imshow(img_grid.permute(1, 2, 0))\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b9e94f-e85d-4794-ba4a-d70bb019b4ea",
      "metadata": {
        "id": "33b9e94f-e85d-4794-ba4a-d70bb019b4ea"
      },
      "source": [
        "## Предобученная диффузионная модель\n",
        "\n",
        "Для дальнейшей работы с разного вида условной генерации нам понадобится предобученная **условная** диффузионная модель. Мы будем использовать простенькую архитектуру, которая была получена скрещиванием CUNet из того же [репозитория](https://github.com/ngushchin/EntropicNeuralOptimalTransport/blob/main/src/cunet.py) и части, кодирующей момент времени и метку класса, из SongUNet в [EDM](https://github.com/NVlabs/edm/blob/main/training/networks.py).\n",
        "\n",
        "Такой выбор был мотивирован следующими наблюдениями:\n",
        "* Готовые качественные архитектуры (те же SongUNet или DhariwalUNet) достаточно долго работают из-за своей глубины, что усложнит решение домашки, в которой, в основном, важны качественные результаты;\n",
        "* Существующие имплементации этих архитектур достаточно абстрактно написаны, чтобы при первом знакомстве было удобно писать для них разного рода надстройки.\n",
        "\n",
        "Архитектура CUNet представляет собой гораздо более легкую и неглубокую модель, за счет чего существенно ускоряет работу с ней и упрощает ее модификацию. Кодирование метки класса и момента времени везде более-менее одинаково (и включает в себя позиционное кодирование/positional encoding), поэтому выбор именно варианта из EDM не существенен."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05672fc9-b070-4dde-bc05-063fdb5a2587",
      "metadata": {
        "id": "05672fc9-b070-4dde-bc05-063fdb5a2587"
      },
      "source": [
        "Как и в семинаре, мы используем надстройку над архитектурой, которая делает все необходимые преобразования над входами: нормирование, взятие логарифма от уровня шума и т.д. Ее имплементация на этот раз взята из репозитория EDM, поэтому загрузка модели выглядит немного необычным образом. Можно не обращать внимания на устройство кода в следующих двух ячейках (небольшая часть кода из гитхаба EDM была изменена для удобства использования в ноутбуке)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc77348-9801-4a09-8c74-843e86926c4b",
      "metadata": {
        "id": "6dc77348-9801-4a09-8c74-843e86926c4b"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/NVlabs/edm\n",
        "!cp edm/training/networks.py edm/training/networks_copy.py\n",
        "!cp fid.py edm/fid.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ad8e287-10bd-4f51-a3d5-54d84b380ef3",
      "metadata": {
        "id": "9ad8e287-10bd-4f51-a3d5-54d84b380ef3"
      },
      "outputs": [],
      "source": [
        "def append_code(in_files, out_file):\n",
        "    lines = ['\\n']\n",
        "    for in_file in in_files:\n",
        "        with open(in_file, 'r') as f:\n",
        "            for line in f:\n",
        "                lines.append(line)\n",
        "\n",
        "    with open(out_file, 'w') as f:\n",
        "        for line in lines:\n",
        "            f.write(line)\n",
        "\n",
        "append_code(['edm/training/networks_copy.py', 'cunet.py'], 'edm/training/networks.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7dd9e75-e220-4388-a7ea-00d4cb48d01e",
      "metadata": {
        "id": "c7dd9e75-e220-4388-a7ea-00d4cb48d01e"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "%cd edm\n",
        "from training.networks import EDMPrecond\n",
        "from torch_utils import misc\n",
        "from dnnlib import util\n",
        "%cd ..\n",
        "\n",
        "cond_model = EDMPrecond(img_resolution=32, img_channels=3, model_type='CUNet', noise_channels=128, base_factor=64, emb_channels=128, label_dim=11)\n",
        "cond_model.eval().cuda()\n",
        "\n",
        "with util.open_url('cond_cunet.pkl') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "misc.copy_params_and_buffers(src_module=data['ema'], dst_module=cond_model, require_all=True)\n",
        "print(f\"Модель имеет {sum(p.numel() for p in cond_model.parameters())} параметров\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89f187df-2c44-4fbf-8be7-fdb660bff5df",
      "metadata": {
        "id": "89f187df-2c44-4fbf-8be7-fdb660bff5df"
      },
      "source": [
        "Возьмем слегка модифицированный код для генерации и визуализации из семинара и посмотрим, как работает модель:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d027ab-1b5c-434b-9068-88f6a69b16b6",
      "metadata": {
        "id": "f2d027ab-1b5c-434b-9068-88f6a69b16b6"
      },
      "outputs": [],
      "source": [
        "def normalize(x):\n",
        "    return x / x.abs().max(dim=0)[0][None, ...]\n",
        "\n",
        "def velocity_from_denoiser(x, model, sigma, class_labels=None, error_eps=1e-4, stochastic=False, cfg=0.0, **model_kwargs):\n",
        "    sigma = sigma[:, None, None, None]\n",
        "    cond_v = (-model(x, sigma, class_labels, **model_kwargs) + x) / (sigma + error_eps)\n",
        "\n",
        "    if cfg > 0.0:\n",
        "        dummy_labels = torch.zeros_like(class_labels)\n",
        "        dummy_labels[:, -1] = 1\n",
        "        uncond_v = (-model(x, sigma, dummy_labels, **model_kwargs) + x) / (sigma + error_eps)\n",
        "        v = cond_v + cfg * (cond_v - uncond_v)\n",
        "    else:\n",
        "        v = cond_v\n",
        "\n",
        "    if stochastic:\n",
        "        v = v * 2\n",
        "\n",
        "    return v\n",
        "\n",
        "def get_timesteps(params):\n",
        "    num_steps = params['num_steps']\n",
        "    sigma_min, sigma_max = params['sigma_min'], params['sigma_max']\n",
        "    rho = params['rho']\n",
        "\n",
        "    step_indices = torch.arange(num_steps, device=params['device'])\n",
        "    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
        "    t_steps = torch.cat([t_steps, torch.zeros_like(t_steps[:1])]) # t_N = 0\n",
        "    return t_steps\n",
        "\n",
        "def sample_euler(model, noise, params, class_labels=None, **model_kwargs):\n",
        "    num_steps = params['num_steps']\n",
        "    vis_steps = params['vis_steps']\n",
        "    t_steps = get_timesteps(params)\n",
        "    x = noise * params['sigma_max']\n",
        "    x_history = [normalize(noise)]\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(t_steps) - 1):\n",
        "            t_cur = t_steps[i]\n",
        "            t_next = t_steps[i + 1]\n",
        "            t_net = t_steps[i] * torch.ones(x.shape[0], device=params['device'])\n",
        "            x = x + velocity_from_denoiser(x, model, t_net, class_labels=class_labels, stochastic=params['stochastic'], cfg=params['cfg'], **model_kwargs) * (t_next - t_cur)\n",
        "            if params['stochastic']:\n",
        "                x = x + torch.randn_like(x) * torch.sqrt(torch.abs(t_next - t_cur) * 2 * t_cur)\n",
        "            x_history.append(normalize(x).view(-1, 3, *x.shape[2:]))\n",
        "\n",
        "    x_history = [x_history[0]] + x_history[::-(num_steps // (vis_steps - 2))][::-1] + [x_history[-1]]\n",
        "\n",
        "    return x, x_history\n",
        "\n",
        "def visualize_model_samples(model, params, labels_usage='dummy', class_labels=None, title='Семплы из модели', **model_kwargs):\n",
        "    noise = torch.randn(40, 3, 32, 32, device=params['device'])\n",
        "    if class_labels is None and labels_usage == 'dummy':\n",
        "        class_labels = torch.zeros(40, 11, device=params['device'])\n",
        "        class_labels[:, -1] = 1\n",
        "    elif labels_usage == 'random':\n",
        "        class_labels = torch.randint(low=0, high=10, size=(40,), device=params['device'])\n",
        "        class_labels = (class_labels[:, None] == torch.arange(11, device=params['device'])[None, :]).float()\n",
        "\n",
        "    out, trajectory = sample_euler(model, noise, params, class_labels=class_labels, **model_kwargs)\n",
        "    out = out * 0.5 + 0.5\n",
        "    visualize_batch(out.detach().cpu(), title=title)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6447e588-a401-44e5-b931-8797f41d3e78",
      "metadata": {
        "id": "6447e588-a401-44e5-b931-8797f41d3e78"
      },
      "source": [
        "Визуализируем условные семплы из модели (мы будем использовать коэффициент classifier-free guidance, равный 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "524a0472-d14e-4d35-916a-bca56610fb37",
      "metadata": {
        "id": "524a0472-d14e-4d35-916a-bca56610fb37"
      },
      "outputs": [],
      "source": [
        "def visualize_cond_samples(model, params, n_samples=3, cfgs=[0.0, 0.5, 1.0, 2.0], **model_kwargs):\n",
        "    fig, ax = plt.subplots(len(cfgs), figsize=(12, 8))\n",
        "    for i in range(len(cfgs)):\n",
        "        remove_ticks(ax[i])\n",
        "        ax[i].set_title('Семплы с коэффициентом cfg = %.4g' % cfgs[i], fontsize=15)\n",
        "\n",
        "    for i in range(len(cfgs)):\n",
        "        cfg = cfgs[i]\n",
        "        noise = torch.randn(n_samples * 10, 3, 32, 32, device=params['device'])\n",
        "        class_labels = torch.eye(n=10, m=11).unsqueeze(0).repeat(n_samples, 1, 1).reshape(-1, 11).float().to(params['device'])\n",
        "        params['cfg'] = cfgs[i]\n",
        "        img, _ = sample_euler(model, noise, params, class_labels=class_labels, **model_kwargs)\n",
        "        img = img * 0.5 + 0.5\n",
        "        img_grid = make_grid(img, nrow=10)\n",
        "        ax[i].imshow(img_grid.permute(1, 2, 0).detach().cpu())\n",
        "\n",
        "sampling_params = {\n",
        "    'device': 'cuda',\n",
        "    'sigma_min': 0.02,\n",
        "    'sigma_max': 80.0,\n",
        "    'num_steps': 50,\n",
        "    'rho': 7.0,\n",
        "    'vis_steps': 1,\n",
        "    'stochastic': False,\n",
        "}\n",
        "\n",
        "visualize_cond_samples(cond_model, sampling_params, cfgs=[0.0, 1.0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d58a4ab9-e0e5-423e-bde2-a862785aa434",
      "metadata": {
        "id": "d58a4ab9-e0e5-423e-bde2-a862785aa434"
      },
      "source": [
        "## Непарные задачи перевода между доменами\n",
        "\n",
        "Задача переноса стиля (перевода между доменами) ставит перед собой построение отображения $G(\\mathbf{X})$ между двумя распределениями $p$ и $q$: то есть, такого отображения, что если $\\mathbf{X} \\sim p$, то $G(\\mathbf{X}) \\sim q$. Любое ли отображение с таким свойством подойдет? Нет, потому что в таком случае мы будем, например, считать решением задачи превращения кошки в собаку отображение $G$, которое переводит кошку в произвольную собаку, не имеющую ничего общего со входным изображением. Нам же хотелось бы гарантировать связь между входом и выходом. Дальше на курсе мы формализуем эту идею с помощью задачи оптимального транспорта.\n",
        "\n",
        "Непарными же считаются задачи, в которых в данных нет явного соответствия между объектами двух доменов (например, в задаче превратить кошку в собаку или мужчину в женщину не очень понятно, как именно выбирать соответствующие пары). Здесь мы просто считаем, что нам дано два независимых набора данных, соответствующих $\\mathbf{X} \\sim p$ и $\\mathbf{Y} \\sim q$.\n",
        "\n",
        "В этой части домашки мы будем решать задачу перевода между распределением $p(\\mathbf{x})$, соответствующим распределению цифр из MNIST и $q(\\mathbf{x})$, соответствующим распределению одного из классов MNIST (например, распределение троек)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9aa525b-89c5-4391-82c0-21dfc8b584f6",
      "metadata": {
        "id": "c9aa525b-89c5-4391-82c0-21dfc8b584f6"
      },
      "source": [
        "## SDEdit\n",
        "\n",
        "Оба метода основываются на одной и той же идее: если у нас есть диффузионная модель, способная генерировать тройки (например, безусловная модель, обученная на тройках, или условная модель, обученная на всем датасете), то чисто теоретически можно превратить любую цифру в тройку следующим образом:\n",
        "* Зашумить цифру $\\mathbf{X}$ до такого уровня $t$, что очертания, позволяющие определить цифру по $\\mathbf{X} + t \\varepsilon$, размываются, но остаются различимыми такие более общие черты, как цвет/толщина и т.д.;\n",
        "* Запустить с помощью \"троечной\" диффузионной модели процесс расшумления, начав его с момента времени $t$ и семпла $\\mathbf{X}_t$.\n",
        "\n",
        "В идеале, генерация с помощью троечной диффузионной модели позволит нам получить правдоподобную тройку, а черты, оставшиеся в картинке после зашумления, позволят на каком-то уровне сохранить стиль исходной цифры. Данный метод называется [SDEdit](https://arxiv.org/abs/2108.01073)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf1e833-7424-4929-8b10-5b98ff661ff1",
      "metadata": {
        "id": "ddf1e833-7424-4929-8b10-5b98ff661ff1"
      },
      "source": [
        "## Задача 1\n",
        "\n",
        "* **(0.2 балла)** Имплементируйте SDEdit для перевода произвольной цифры в цифру фиксированного класса (передав соответствующую метку в предобученную условную модель). Обратите внимание, что наша имплементация семплинга по схеме Эйлера ждет на вход $X_t / t$, поскольку в коде вход умножается на $t$;\n",
        "* **(0.2 балла)** Проанализируйте, как меняется качество работы модели при изменении ее единственного гиперпараметра — уровня шума $t$, который прибавляется к исходной цифре. Возьмите по одной цифре из каждого класса и визуализируйте выходы метода при разных $t$. Поэкспериментируйте с разными $t$, чтобы визуализировать такие $t$, между которыми качественно меняется работа метода (например, сравнивать метод на $t = 79.0$ и $t = 80.0$ нет смысла). Как при изменении $t$ изменяется качество семплов и их похожесть на вход? Какой уровень шума $t$ вы бы предложили использовать?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "020614bd-2c0f-4b5c-852d-b39838d8cc49",
      "metadata": {
        "id": "020614bd-2c0f-4b5c-852d-b39838d8cc49"
      },
      "outputs": [],
      "source": [
        "# params: параметры семплинга по Эйлеру\n",
        "# x_source: исходная картинка\n",
        "# target_label: число, метка класса, в который нужно превратить объекты на входе\n",
        "# гиперпараметр t передается в метод как params['sigma_max']\n",
        "\n",
        "def sdedit(model, x_source, target_label, params):\n",
        "    out = ...\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67b8269a-4c16-44dc-8361-2aa58f1fb2a8",
      "metadata": {
        "id": "67b8269a-4c16-44dc-8361-2aa58f1fb2a8"
      },
      "outputs": [],
      "source": [
        "def visualize_transform(batch, batch_out):\n",
        "    batch_cat = torch.cat((batch, batch_out), dim=0)\n",
        "    image_grid = make_grid(batch_cat.cpu(), nrow=len(batch))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(3 * len(batch), 3))\n",
        "    remove_ticks(ax)\n",
        "    ax.imshow(image_grid.permute(1, 2, 0))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1016a932-8e6e-4703-b6b3-00654f78a458",
      "metadata": {
        "id": "1016a932-8e6e-4703-b6b3-00654f78a458"
      },
      "outputs": [],
      "source": [
        "sampling_params = {\n",
        "    'device': 'cuda',\n",
        "    'sigma_min': 0.02,\n",
        "    'sigma_max': ...,\n",
        "    'num_steps': 50,\n",
        "    'rho': 7.0,\n",
        "    'vis_steps': 1,\n",
        "    'stochastic': False,\n",
        "    'cfg': 1.0\n",
        "}\n",
        "\n",
        "x_source = (next(iter(train_dataloader))[0] * 2 - 1).cuda()[:10]\n",
        "x_out = sdedit(cond_model, x_source, target_label=3, params=sampling_params)\n",
        "visualize_transform(x_source, x_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c276115a-3a5c-4eda-8236-fcfc41f137ba",
      "metadata": {
        "id": "c276115a-3a5c-4eda-8236-fcfc41f137ba"
      },
      "source": [
        "## DDIB (Dual Diffusion Implicit Bridges)\n",
        "\n",
        "Следующий метод, который мы рассмотрим, называется Dual Diffusion Implicit Bridges [(DDIB)](https://arxiv.org/abs/2203.08382). Он использует ту же идею, что SDEdit (зашумить семпл из исходного домена и расшумить его диффузионной моделью для целевого домена), но делает это более умно. Принципиальная проблема SDEdit состоит в том, что стохастическое зашумление всегда сопровождается потерей данных об исходном изображении (если $t$ слишком большое, то можно считать, что информации вообще никакой не остается). Если бы был способ детерминированного зашумления данных, это бы решило проблему, так как позволило бы превратить вход во что-то, что может принять на вход диффузионная модель для таргетного домена.\n",
        "\n",
        "А такой способ у нас есть! Подойдет представление диффузионных моделей через обыкновенные дифференциальные уравнения: прямой процесс зашумления\n",
        "$$\n",
        "    \\mathrm{d} \\mathbf{X}_t = g(t) \\mathrm{d} \\mathbf{W}_t\n",
        "$$\n",
        "эквивалентен ОДУ\n",
        "$$\n",
        "    \\mathrm{d} \\mathbf{Y}_t = -\\frac{g^2(t)}{2} \\nabla \\log p_t(\\mathbf{Y}_t) \\mathrm{d} t\n",
        "$$\n",
        "с точки зрения маргинальных распределений в каждый момент времени $t$. Тогда зашумить изображение из исходного домена можно решив соответствующее ОДУ с момента времени $0$ до момента времени $t$. Все, что для этого нужно, — иметь диффузионную модель для исходного домена (а такая модель в нашем сеттинге с условной моделью на MNIST'e есть).  Полученное зашумленное изображение, как и раньше, подается в диффузионную модель для целевого домена и расшумляется."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7773272f-ff14-4eed-b57c-0b0f9d03bec9",
      "metadata": {
        "id": "7773272f-ff14-4eed-b57c-0b0f9d03bec9"
      },
      "source": [
        "## Задача 2\n",
        "* **(0.3 балла)** Реализуйте детерминированное зашумление изображения с помощью метода Эйлера, взяв $\\sigma_t = t$ и $g(t) = \\sqrt{2t}$ (именно эти параметры мы взяли в 6 лекции, с ними имплементировали схему Эйлера, которую потом скопировали во все 3 части домашки). На его основе релизуйте метод DDIB. Так как работаем мы с переводом произвольной цифры в, например, тройку, при зашумлении изображения мы будем подавать на вход сети его метку класса, а при расшумлении — метку целевого класса. При зашумлении тоже имеет смысл использовать CFG.\n",
        "\n",
        "* Как и в SDEdit, обратите внимание, что наша имплементация семплинга по схеме Эйлера ждет на вход $X_t / t$, поскольку в коде вход умножается на $t$;\n",
        "\n",
        "* **(0.1 балл)** Визуализируйте траекторию зашумления и расшумления изображения при выборе максимального $t = T$ (80.0 в наших экспериментах). Похож ли \"детерминированный шум\", полученный при кодировании входа, на семпл из нормального распределения? При визуализации траектории имеет смысл нормировать промежуточные изображения (например, так, как это делается в лекции 5).\n",
        "\n",
        "  \n",
        "* **(0.2 балла, копипаста из SDEdit)** Проанализируйте, как меняется качество работы модели при изменении ее единственного гиперпараметра — уровня шума $t$, до которого кодируется исходная картинка. Возьмите по одной цифре из каждого класса и визуализируйте выходы метода при разных $t$. Поэкспериментируйте с разными $t$, чтобы визуализировать такие $t$, между которыми качественно меняется работа метода. Как при изменении $t$ изменяется качество семплов и их похожесть на вход? Какой уровень шума $t$ вы бы предложили использовать?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce821528-f592-447d-af9c-74e7b4015039",
      "metadata": {
        "id": "ce821528-f592-447d-af9c-74e7b4015039"
      },
      "outputs": [],
      "source": [
        "sampling_params = {\n",
        "    'device': 'cuda',\n",
        "    'sigma_min': 0.02,\n",
        "    'sigma_max': ...,\n",
        "    'num_steps': 50,\n",
        "    'rho': 7.0,\n",
        "    'vis_steps': 1,\n",
        "    'cfg': 1.0\n",
        "}\n",
        "\n",
        "def encode_euler(model, x_source, params, class_labels=None, **model_kwargs):\n",
        "    num_steps = params['num_steps']\n",
        "    vis_steps = params['vis_steps']\n",
        "    t_steps = get_timesteps(params) # здесь t убывают!\n",
        "    x = x_source\n",
        "\n",
        "    x_history = [normalize(x)]\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(t_steps) - 1):\n",
        "            x = x + ...\n",
        "\n",
        "            x_history.append(normalize(x).view(-1, 3, *x.shape[2:]))\n",
        "\n",
        "    x_history = [x_history[0]] + x_history[::-(num_steps // (vis_steps - 2))][::-1] + [x_history[-1]]\n",
        "\n",
        "    return x, x_history\n",
        "\n",
        "def ddib(model, x_source, target_label, params):\n",
        "    out = ...\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c158ae2-8f3b-4425-813a-1ad7b21e2028",
      "metadata": {
        "id": "2c158ae2-8f3b-4425-813a-1ad7b21e2028"
      },
      "source": [
        "## Задача 3 (бонус, 0.5 балла)\n",
        "\n",
        "Возьмите по 3-5 (адекватных) значений $t$ для каждого из двух методов. Для каждого $t$ запустите метод (с тройками в качестве целевого домена) на подмножестве тестового датасета MNIST (1000-2000 картинок) и посчитайте две метрики:\n",
        "* Средняя по датасету \"похожесть\" между входом и выходом, посчитанная как попиксельная $L_2$ норма разности;\n",
        "* FID между сгенерированными тройками и тройками из трейн датасета (предпосчитанные статистики лежат в *cmnist_train_3.npz*).\n",
        "\n",
        "Визуализируйте полученные метрики в виде двумерного scatter plot с осями, соответствующими метрикам, и прокомментируйте результаты. Какой из методов достигает лучшего баланса между качеством семплов и похожестью входа на выход?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}