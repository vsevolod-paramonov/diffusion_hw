{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6e4ca8ea-74b8-424f-b349-7b89a651a0a9",
      "metadata": {
        "id": "6e4ca8ea-74b8-424f-b349-7b89a651a0a9"
      },
      "source": [
        "# 1. Classifier-free guidance в ограниченном промежутке"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e8bfd3-b7aa-4d96-a4c3-b00dbba672fe",
      "metadata": {
        "id": "08e8bfd3-b7aa-4d96-a4c3-b00dbba672fe"
      },
      "source": [
        "На лекции с ноутбуком мы познакомились с Classifier-free Guidance (CFG): техникой для балансирования между качеством и разнообразием генерации из условных диффузионных моделей. Минусом этой схемы является увеличение числа вызовов нейросети в 2 раза по сравнению с обычной условной генерацией. В этой части домашки мы попробуем уменьшить количество применений CFG с сохранением (или даже улучшением) качества генерации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84cf8a9f-b6af-4ed0-9be2-75d4223cbd12",
      "metadata": {
        "id": "84cf8a9f-b6af-4ed0-9be2-75d4223cbd12"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c651c37a-752f-4c02-9a51-6b26a28496a9",
      "metadata": {
        "id": "c651c37a-752f-4c02-9a51-6b26a28496a9"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms import Compose, Resize, ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0ba65d-c479-4ef4-a035-a92e8f735f74",
      "metadata": {
        "id": "1c0ba65d-c479-4ef4-a035-a92e8f735f74"
      },
      "source": [
        "## Цветной MNIST\n",
        "\n",
        "В домашке предлагается поработать с цветной модификацией датасета MNIST (код для покраски взят [у коллег](https://github.com/ngushchin/EntropicNeuralOptimalTransport/blob/main/src/tools.py) из Сколтеха). С одной стороны, такой датасет все еще оставляет возможность обучать диффузионные модели, но делает свойства модели более интерпретируемыми (например, в задачах условной генерации, таких, как дорисовывание, повышение разрешения и деблюринг, можно отследить корректное сохранение цвета изображения)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5ba6253-ee32-4c0c-9015-4aed7cd61680",
      "metadata": {
        "id": "d5ba6253-ee32-4c0c-9015-4aed7cd61680"
      },
      "outputs": [],
      "source": [
        "class ColoredMNIST(MNIST):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hues = 360 * torch.rand(super().__len__())\n",
        "\n",
        "    def __len__(self):\n",
        "        return super().__len__()\n",
        "\n",
        "    def color_image(self, img, idx):\n",
        "        img_min = 0\n",
        "        a = (img - img_min) * (self.hues[idx] % 60) / 60\n",
        "        img_inc = a\n",
        "        img_dec = img - a\n",
        "\n",
        "        colored_image = torch.zeros((3, img.shape[1], img.shape[2]))\n",
        "        H_i = round(self.hues[idx].item() / 60) % 6\n",
        "\n",
        "        if H_i == 0:\n",
        "            colored_image[0] = img\n",
        "            colored_image[1] = img_inc\n",
        "            colored_image[2] = img_min\n",
        "        elif H_i == 1:\n",
        "            colored_image[0] = img_dec\n",
        "            colored_image[1] = img\n",
        "            colored_image[2] = img_min\n",
        "        elif H_i == 2:\n",
        "            colored_image[0] = img_min\n",
        "            colored_image[1] = img\n",
        "            colored_image[2] = img_inc\n",
        "        elif H_i == 3:\n",
        "            colored_image[0] = img_min\n",
        "            colored_image[1] = img_dec\n",
        "            colored_image[2] = img\n",
        "        elif H_i == 4:\n",
        "            colored_image[0] = img_inc\n",
        "            colored_image[1] = img_min\n",
        "            colored_image[2] = img\n",
        "        elif H_i == 5:\n",
        "            colored_image[0] = img\n",
        "            colored_image[1] = img_min\n",
        "            colored_image[2] = img_dec\n",
        "\n",
        "        return colored_image\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = super().__getitem__(idx)\n",
        "        return self.color_image(img, idx), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9659fca4-809c-4513-907b-6941c96f82df",
      "metadata": {
        "id": "9659fca4-809c-4513-907b-6941c96f82df"
      },
      "outputs": [],
      "source": [
        "transform = Compose([Resize((32, 32)), ToTensor()])\n",
        "data_train = ColoredMNIST(root='.', train=True, download=False, transform=transform)\n",
        "# раскомментируйте, чтобы скачать\n",
        "#data_train = ColoredMNIST(root='.', train=True, download=True, transform=transform)\n",
        "\n",
        "data_test = ColoredMNIST(root='.', train=False, download=False, transform=transform)\n",
        "train_dataloader = DataLoader(data_train, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(data_test, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f224c9-016a-4cdd-9bef-45ee4febc2be",
      "metadata": {
        "id": "10f224c9-016a-4cdd-9bef-45ee4febc2be"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "def remove_ticks(ax):\n",
        "    ax.tick_params(\n",
        "        axis='both',\n",
        "        which='both',\n",
        "        bottom=False,\n",
        "        top=False,\n",
        "        labelbottom=False,\n",
        "        left=False,\n",
        "        labelleft=False\n",
        "    )\n",
        "\n",
        "def remove_xticks(ax):\n",
        "    ax.tick_params(\n",
        "        axis='both',\n",
        "        which='both',\n",
        "        bottom=False,\n",
        "        top=False,\n",
        "        labelbottom=False,\n",
        "        left=True,\n",
        "        labelleft=True\n",
        "    )\n",
        "\n",
        "def visualize_batch(img_vis, title='Семплы из цветного MNIST', nrow=10, ncol=4):\n",
        "    img_grid = make_grid(img_vis, nrow=nrow)\n",
        "    fig, ax = plt.subplots(1, figsize=(nrow, ncol))\n",
        "    remove_ticks(ax)\n",
        "    ax.set_title(title, fontsize=14)\n",
        "    ax.imshow(img_grid.permute(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_batch(next(iter(train_dataloader))[0][:40])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b9e94f-e85d-4794-ba4a-d70bb019b4ea",
      "metadata": {
        "id": "33b9e94f-e85d-4794-ba4a-d70bb019b4ea"
      },
      "source": [
        "## Предобученная диффузионная модель\n",
        "\n",
        "Для дальнейшей работы с разного вида условной генерации нам понадобится предобученная **условная** диффузионная модель. Мы будем использовать простенькую архитектуру, которая была получена скрещиванием CUNet из того же [репозитория](https://github.com/ngushchin/EntropicNeuralOptimalTransport/blob/main/src/cunet.py) и части, кодирующей момент времени и метку класса, из SongUNet в [EDM](https://github.com/NVlabs/edm/blob/main/training/networks.py).\n",
        "\n",
        "Такой выбор был мотивирован следующими наблюдениями:\n",
        "* Готовые качественные архитектуры (те же SongUNet или DhariwalUNet) достаточно долго работают из-за своей глубины, что усложнит решение домашки, в которой, в основном, важны качественные результаты;\n",
        "* Существующие имплементации этих архитектур достаточно абстрактно написаны, чтобы при первом знакомстве было удобно писать для них разного рода надстройки.\n",
        "\n",
        "Архитектура CUNet представляет собой гораздо более легкую и неглубокую модель, за счет чего существенно ускоряет работу с ней и упрощает ее модификацию. Кодирование метки класса и момента времени везде более-менее одинаково (и включает в себя позиционное кодирование/positional encoding), поэтому выбор именно варианта из EDM не существенен."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05672fc9-b070-4dde-bc05-063fdb5a2587",
      "metadata": {
        "id": "05672fc9-b070-4dde-bc05-063fdb5a2587"
      },
      "source": [
        "Как и в семинаре, мы используем надстройку над архитектурой, которая делает все необходимые преобразования над входами: нормирование, взятие логарифма от уровня шума и т.д. Ее имплементация на этот раз взята из репозитория EDM, поэтому загрузка модели выглядит немного необычным образом. Можно не обращать внимания на устройство кода в следующих двух ячейках (небольшая часть кода из гитхаба EDM была изменена для удобства использования в ноутбуке)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc77348-9801-4a09-8c74-843e86926c4b",
      "metadata": {
        "id": "6dc77348-9801-4a09-8c74-843e86926c4b"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/NVlabs/edm\n",
        "!cp edm/training/networks.py edm/training/networks_copy.py\n",
        "!cp fid.py edm/fid.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ad8e287-10bd-4f51-a3d5-54d84b380ef3",
      "metadata": {
        "id": "9ad8e287-10bd-4f51-a3d5-54d84b380ef3"
      },
      "outputs": [],
      "source": [
        "def append_code(in_files, out_file):\n",
        "    lines = ['\\n']\n",
        "    for in_file in in_files:\n",
        "        with open(in_file, 'r') as f:\n",
        "            for line in f:\n",
        "                lines.append(line)\n",
        "\n",
        "    with open(out_file, 'w') as f:\n",
        "        for line in lines:\n",
        "            f.write(line)\n",
        "\n",
        "append_code(['edm/training/networks_copy.py', 'cunet.py'], 'edm/training/networks.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7dd9e75-e220-4388-a7ea-00d4cb48d01e",
      "metadata": {
        "id": "c7dd9e75-e220-4388-a7ea-00d4cb48d01e"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "%cd edm\n",
        "from training.networks import EDMPrecond\n",
        "from torch_utils import misc\n",
        "from dnnlib import util\n",
        "%cd ..\n",
        "\n",
        "cond_model = EDMPrecond(img_resolution=32, img_channels=3, model_type='CUNet', noise_channels=128, base_factor=64, emb_channels=128, label_dim=11)\n",
        "cond_model.eval().cuda()\n",
        "\n",
        "with util.open_url('cond_cunet.pkl') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "misc.copy_params_and_buffers(src_module=data['ema'], dst_module=cond_model, require_all=True)\n",
        "print(f\"Модель имеет {sum(p.numel() for p in cond_model.parameters())} параметров\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89f187df-2c44-4fbf-8be7-fdb660bff5df",
      "metadata": {
        "id": "89f187df-2c44-4fbf-8be7-fdb660bff5df"
      },
      "source": [
        "Возьмем слегка модифицированный код для генерации и визуализации из семинара и посмотрим, как работает модель. Здесь в схеме Эйлера появились два новых гиперпараметра: **iter_start** и **iter_end**, соответствующие началу и концу отрезка, на котором используется CFG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d027ab-1b5c-434b-9068-88f6a69b16b6",
      "metadata": {
        "id": "f2d027ab-1b5c-434b-9068-88f6a69b16b6"
      },
      "outputs": [],
      "source": [
        "def normalize(x):\n",
        "    return x / x.abs().max(dim=0)[0][None, ...]\n",
        "\n",
        "def velocity_from_denoiser(x, model, sigma, class_labels=None, error_eps=1e-4, stochastic=False, cfg=0.0, **model_kwargs):\n",
        "    sigma = sigma[:, None, None, None]\n",
        "    cond_v = (-model(x, sigma, class_labels, **model_kwargs) + x) / (sigma + error_eps)\n",
        "\n",
        "    if cfg > 0.0:\n",
        "        dummy_labels = torch.zeros_like(class_labels)\n",
        "        dummy_labels[:, -1] = 1\n",
        "        uncond_v = (-model(x, sigma, dummy_labels, **model_kwargs) + x) / (sigma + error_eps)\n",
        "        v = cond_v + cfg * (cond_v - uncond_v)\n",
        "    else:\n",
        "        v = cond_v\n",
        "\n",
        "    if stochastic:\n",
        "        v = v * 2\n",
        "\n",
        "    return v\n",
        "\n",
        "def get_timesteps(params):\n",
        "    num_steps = params['num_steps']\n",
        "    sigma_min, sigma_max = params['sigma_min'], params['sigma_max']\n",
        "    rho = params['rho']\n",
        "\n",
        "    step_indices = torch.arange(num_steps, device=params['device'])\n",
        "    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
        "    t_steps = torch.cat([t_steps, torch.zeros_like(t_steps[:1])]) # t_N = 0\n",
        "    return t_steps\n",
        "\n",
        "def sample_euler(model, noise, params, class_labels=None, **model_kwargs):\n",
        "    num_steps = params['num_steps']\n",
        "    vis_steps = params['vis_steps']\n",
        "    t_steps = get_timesteps(params)\n",
        "    x = noise * params['sigma_max']\n",
        "    x_history = [normalize(noise)]\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(t_steps) - 1):\n",
        "            t_cur = t_steps[i]\n",
        "            t_next = t_steps[i + 1]\n",
        "            t_net = t_steps[i] * torch.ones(x.shape[0], device=params['device'])\n",
        "            if (i >= params['iter_start'] and i <= params['iter_end']):\n",
        "                x = x + velocity_from_denoiser(x, model, t_net, class_labels=class_labels, stochastic=params['stochastic'], cfg=params['cfg'], **model_kwargs) * (t_next - t_cur)\n",
        "            else:\n",
        "                x = x + velocity_from_denoiser(x, model, t_net, class_labels=class_labels, stochastic=params['stochastic'], cfg=0.0, **model_kwargs) * (t_next - t_cur)\n",
        "\n",
        "            if params['stochastic']:\n",
        "                x = x + torch.randn_like(x) * torch.sqrt(torch.abs(t_next - t_cur) * 2 * t_cur)\n",
        "            x_history.append(normalize(x).view(-1, 3, *x.shape[2:]))\n",
        "\n",
        "    x_history = [x_history[0]] + x_history[::-(num_steps // (vis_steps - 2))][::-1] + [x_history[-1]]\n",
        "\n",
        "    return x, x_history\n",
        "\n",
        "def visualize_model_samples(model, params, labels_usage='dummy', class_labels=None, title='Семплы из модели', **model_kwargs):\n",
        "    noise = torch.randn(40, 3, 32, 32, device=params['device'])\n",
        "    if class_labels is None and labels_usage == 'dummy':\n",
        "        class_labels = torch.zeros(40, 11, device=params['device'])\n",
        "        class_labels[:, -1] = 1\n",
        "    elif labels_usage == 'random':\n",
        "        class_labels = torch.randint(low=0, high=10, size=(40,), device=params['device'])\n",
        "        class_labels = (class_labels[:, None] == torch.arange(11, device=params['device'])[None, :]).float()\n",
        "\n",
        "    out, trajectory = sample_euler(model, noise, params, class_labels=class_labels, **model_kwargs)\n",
        "    out = out * 0.5 + 0.5\n",
        "    visualize_batch(out.detach().cpu(), title=title)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e10f1451-7857-4e60-9295-0faff883bc1f",
      "metadata": {
        "id": "e10f1451-7857-4e60-9295-0faff883bc1f"
      },
      "source": [
        "Безусловную генерацию из условной модели можно реализовать двумя способами: подав метку фиктивного класса (как в classifier-free guidance) или сгенерировав случайную метку класса и подав ее на вход. Сгенерируем обоими способами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5b82661-a653-46f6-a1bb-2b2920b27e41",
      "metadata": {
        "id": "f5b82661-a653-46f6-a1bb-2b2920b27e41"
      },
      "outputs": [],
      "source": [
        "sampling_params = {\n",
        "    'device': 'cuda',\n",
        "    'sigma_min': 0.02,\n",
        "    'sigma_max': 80.0,\n",
        "    'num_steps': 50,\n",
        "    'rho': 7.0,\n",
        "    'vis_steps': 1,\n",
        "    'stochastic': False,\n",
        "    'cfg': 0.0,\n",
        "    'iter_start': 0,\n",
        "    'iter_end': 50\n",
        "}\n",
        "\n",
        "visualize_model_samples(cond_model, params=sampling_params, labels_usage='dummy', title='Семплы из модели, обусловленной на dummy класс')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "256f2e5e-34b2-44fc-8b14-cebc572f3907",
      "metadata": {
        "id": "256f2e5e-34b2-44fc-8b14-cebc572f3907"
      },
      "outputs": [],
      "source": [
        "visualize_model_samples(cond_model, params=sampling_params, labels_usage='random', title='Семплы из модели, обусловленной на случайный класс')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6447e588-a401-44e5-b931-8797f41d3e78",
      "metadata": {
        "id": "6447e588-a401-44e5-b931-8797f41d3e78"
      },
      "source": [
        "Также визуализируем условные семплы из модели с разными коэффициентами classifier-free guidance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "524a0472-d14e-4d35-916a-bca56610fb37",
      "metadata": {
        "id": "524a0472-d14e-4d35-916a-bca56610fb37"
      },
      "outputs": [],
      "source": [
        "def visualize_cond_samples(model, params, n_samples=3, cfgs=[0.0, 0.5, 2.0], **model_kwargs):\n",
        "    fig, ax = plt.subplots(len(cfgs), figsize=(12, 12))\n",
        "    for i in range(len(cfgs)):\n",
        "        remove_ticks(ax[i])\n",
        "        ax[i].set_title('Семплы с коэффициентом cfg = %.4g' % cfgs[i], fontsize=15)\n",
        "\n",
        "    for i in range(len(cfgs)):\n",
        "        cfg = cfgs[i]\n",
        "        noise = torch.randn(n_samples * 10, 3, 32, 32, device=params['device'])\n",
        "        class_labels = torch.eye(n=10, m=11).unsqueeze(0).repeat(n_samples, 1, 1).reshape(-1, 11).float().to(params['device'])\n",
        "        params['cfg'] = cfgs[i]\n",
        "        img, _ = sample_euler(model, noise, params, class_labels=class_labels, **model_kwargs)\n",
        "        img = img * 0.5 + 0.5\n",
        "        img_grid = make_grid(img, nrow=10)\n",
        "        ax[i].imshow(img_grid.permute(1, 2, 0).detach().cpu())\n",
        "\n",
        "sampling_params = {\n",
        "    'device': 'cuda',\n",
        "    'sigma_min': 0.02,\n",
        "    'sigma_max': 80.0,\n",
        "    'num_steps': 50,\n",
        "    'rho': 7.0,\n",
        "    'vis_steps': 1,\n",
        "    'stochastic': False,\n",
        "    'iter_start': 0,\n",
        "    'iter_end': 50\n",
        "}\n",
        "\n",
        "visualize_cond_samples(cond_model, sampling_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5ef9230-2a89-49e4-98f1-d29103e6b04e",
      "metadata": {
        "id": "e5ef9230-2a89-49e4-98f1-d29103e6b04e"
      },
      "source": [
        "## Метрики качества генерации\n",
        "\n",
        "Для домашки пригодится мерить качество моделей генерации. Как это сделать? Сходу не очень очевидно, если поставлена задача безусловной генерации, в которой нет ground truth сопоставления входа выходу. В такой ситуации стоит вспомнить, что задача генеративного моделирования, в первую очередь, состоит в приближении моделью распределения датасета. Разумной метрикой качества послужит какое-нибудь расстояние между распределением данных и распределением, порожденным генеративной моделью."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc691c24-c447-434f-bb76-2fc0d92600ca",
      "metadata": {
        "id": "fc691c24-c447-434f-bb76-2fc0d92600ca"
      },
      "source": [
        "\n",
        "Распределениями мы, к сожалению, не располагаем, зато располагаем обучающим датасетом $\\boldsymbol{X}_1, \\ldots, \\boldsymbol{X}_N$ и сгенерированной выборкой $\\boldsymbol{Y}_1, \\ldots, \\boldsymbol{Y}_M$, поэтому нужно прибегать к оценке расстояния между распределениями по семплам. Беда состоит в том, что в высокой размерности классические методы восстановления плотности (метод гистограмм, ядерная оценка плотности (KDE) и т.д.), которые можно было бы применить к данным, чтобы потом получить две плотности $\\hat{p}_{\\boldsymbol{X}}$ и $\\hat{p}_{\\boldsymbol{Y}}$, работают очень плохо. Помимо этого, приблизить два набора данных некоторым простым распределением типа гауссовского тоже не самая корректная процедура: картиночные данные принципиально многомодальные, и такое приближение будет плохим."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b1943c-2de2-4165-bd88-da236ce088f2",
      "metadata": {
        "id": "f1b1943c-2de2-4165-bd88-da236ce088f2"
      },
      "source": [
        "На основе этих наблюдений была предложена следующая идея. Во-первых, данные можно \"упростить\", если перевести их в некоторое семантическое пространство с более осмысленным расстоянием, чем попиксельное $L_2$ между картинками. Есть шанс, что такое преобразование позволит сгруппировать данные и сделать распределение ближе к унимодальному (особенно, если при этом размерность снижается). В качестве такого пространства предлагается взять нейросеть, \"видевшую все\", обученную на классификацию какого-нибудь большого датасета, и взять ее признаковое пространство. Таким образом, наши новые данные — $f(\\boldsymbol{X}_1), \\ldots, f(\\boldsymbol{X}_N)$ и $f(\\boldsymbol{Y}_1), \\ldots, f(\\boldsymbol{Y}_M)$, где $f$ — функция, соответсвующая выходу нейросети перед классификационным слоем (который как раз отвечает за выделенные в объекте признаки). Исторически в качестве такой нейросети используется [InceptionV3](https://arxiv.org/abs/1512.00567), обученная на классификацию [ImageNet](https://www.image-net.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d7448e-0ffd-414d-ade7-265474e6e90a",
      "metadata": {
        "id": "81d7448e-0ffd-414d-ade7-265474e6e90a"
      },
      "source": [
        "Метрика [Fréchet Inception Distance (FID)](https://arxiv.org/abs/1706.08500), построенная на данной идее, заканчивает ее максимально просто. Оба набора данных приближаются гауссовскими распределениями с параметрами, равными соответствующим выборочным среднему и дисперсии:\n",
        "$$\n",
        "    \\hat{p}_{f(\\boldsymbol{X})}(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{z} \\mid \\mu_{\\boldsymbol{X}}, \\Sigma_{\\boldsymbol{X}}); \\:\\:\\:\\:\\: \\hat{p}_{f(\\boldsymbol{Y})}(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{z} \\mid \\mu_{\\boldsymbol{Y}}, \\Sigma_{\\boldsymbol{Y}}),\n",
        "$$\n",
        "$$\n",
        "    \\mu_{\\boldsymbol{X}} = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} f(\\boldsymbol{X}_i), \\:\\:\\:\\: \\Sigma_{\\boldsymbol{X}} = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} \\left(f(\\boldsymbol{X}_i) - \\mu_{\\boldsymbol{X}} \\right)\\left(f(\\boldsymbol{X}_i) - \\mu_{\\boldsymbol{X}} \\right)^{\\top}\n",
        "$$\n",
        "$$\n",
        "    \\mu_{\\boldsymbol{Y}} = \\frac{1}{M} \\sum\\limits_{j = 1}^{M} f(\\boldsymbol{Y}_j), \\:\\:\\:\\: \\Sigma_{\\boldsymbol{Y}} = \\frac{1}{M} \\sum\\limits_{j = 1}^{M} \\left(f(\\boldsymbol{Y}_j) - \\mu_{\\boldsymbol{Y}} \\right)\\left(f(\\boldsymbol{Y}_j) - \\mu_{\\boldsymbol{Y}} \\right)^{\\top}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00eaab9c-3347-4f49-bfcb-38464f78a0e1",
      "metadata": {
        "id": "00eaab9c-3347-4f49-bfcb-38464f78a0e1"
      },
      "source": [
        "После таких манипуляций получились два распределения с явно заданной плотностью, между которыми уже можно вычислить некоторое расстояние. Одним из известных расстояний между распределениями является расстояние Вассерштейна, которое тесно связано с задачей оптимального транспорта и еще будет упомянуто в курсе. Сейчас же для нас важно то, что у такого расстояния есть явно заданная формула:\n",
        "\n",
        "$$\n",
        "    \\mathcal{W}_2 (\\mathcal{N}(\\mu_{\\boldsymbol{X}}, \\Sigma_{\\boldsymbol{X}}), \\mathcal{N}(\\mu_{\\boldsymbol{Y}}, \\Sigma_{\\boldsymbol{Y}})) = \\|\\mu_{\\boldsymbol{X}} - \\mu_{\\boldsymbol{Y}}\\|^2 + \\text{Tr}\\left(\\Sigma_{\\boldsymbol{X}} + \\Sigma_{\\boldsymbol{Y}} - 2 \\left(\\Sigma_{\\boldsymbol{X}} \\Sigma_{\\boldsymbol{Y}} \\right)^{\\frac{1}{2}} \\right) = \\|\\mu_{\\boldsymbol{X}} - \\mu_{\\boldsymbol{Y}}\\|^2 + \\|\\sqrt{\\Sigma_{\\boldsymbol{X}}} - \\sqrt{\\Sigma_{\\boldsymbol{Y}}} \\|_{\\text{F}}^2.\n",
        "$$\n",
        "\n",
        "Ее смысл очень простой: расстояние между двумя нормальными распределениями считается как квадрат расстояния между их средними плюс квадрат расстояния между корнями из матриц ковариации (в одномерном случае это квадрат расстояния между стандартными отклонениями)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a65c5fc-f75e-4868-853b-642c792ca373",
      "metadata": {
        "id": "3a65c5fc-f75e-4868-853b-642c792ca373"
      },
      "source": [
        "Есть разные подходы к подсчету FID'a для разных задач, но в задаче безусловной генерации обычно считают то, насколько близки статистики сгенерированных изображений (часто берут 50000) к статистикам трейн датасета. Таким образом, мы измеряем то, насколько хорошо модель выучила распределение обучающих данных. Метрика достаточно чувствительна к количеству семплов и имеет тенденцию к недооценке реального расстояния, если взято небольшое количество семплов (проблемы во многом лезут из матрицы ковариаций, которая вырождена всегда, когда семплов меньше, чем их размерность). Тем не менее, на MNIST'е даже такое небольшое количество семплов, как **3000**, дает адекватное приближение. Здесь мы посчитаем FID на **3000** семплах, чтобы экономить время (такое количество семплов дает переоценку примерно на один-два пункта по сравнению с полноценным количеством семплов)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a94796c7-511a-4770-8987-0024369aedbd",
      "metadata": {
        "id": "a94796c7-511a-4770-8987-0024369aedbd"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# сохраним семплы из модели для дальнейшего подсчета FID\n",
        "# labels_usage: если dummy, то мы передаем в модель класс 10, который соответствует безусловной генерации\n",
        "# если uniform, то мы генерируем по num_samples / 10 изображений каждого класса\n",
        "\n",
        "\n",
        "def save_model_samples(name, model, params, batch_size, num_samples, labels_usage='dummy', **model_kwargs):\n",
        "    if os.path.exists(name):\n",
        "        shutil.rmtree(name) # перед сохранением изображений в папку папка очищается\n",
        "    os.makedirs(name, exist_ok=True) # и пересоздается\n",
        "    count = 0\n",
        "\n",
        "    assert num_samples % 10 == 0\n",
        "\n",
        "    with tqdm(total= num_samples) as pbar:\n",
        "        while count < num_samples:\n",
        "            cur_batch_size = min(num_samples - count, batch_size)\n",
        "            noise = torch.randn(cur_batch_size, 3, 32, 32, device=params['device'])\n",
        "            if labels_usage == 'dummy':\n",
        "                labels = torch.zeros(cur_batch_size, 11, device=params['device'])\n",
        "                labels[:, -1] = 1\n",
        "            elif labels_usage == 'uniform':\n",
        "                idxs = torch.arange(count, count + cur_batch_size, device=params['device'])\n",
        "                labels = idxs // (num_samples / 10)\n",
        "                labels = (labels[:, None] == torch.arange(11, device=labels.device)[None, :]).float()\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "            out, trajectory = sample_euler(model, noise, params, class_labels=labels, **model_kwargs)\n",
        "            out = (out * 127.5 + 128).clip(0, 255).to(torch.uint8).permute(0, 2, 3, 1).cpu().numpy()\n",
        "            for i in range(out.shape[0]):\n",
        "                img = Image.fromarray(out[i])\n",
        "                n_digits = len(str(count))\n",
        "                img_name = (6 - n_digits) * '0' + str(count) + '.png'\n",
        "                img.save(os.path.join(name, img_name))\n",
        "                count += 1\n",
        "                pbar.update(1)\n",
        "                pbar.set_description('%d images saved' % (count,))\n",
        "\n",
        "# возьмем реализацию подсчета FID из EDM\n",
        "%cd edm\n",
        "from fid import calculate_inception_stats, calculate_fid_from_inception_stats\n",
        "from dnnlib.util import open_url\n",
        "%cd ..\n",
        "\n",
        "def calc_fid(image_path, ref_path, num_expected, batch):\n",
        "    with open_url(ref_path) as f:\n",
        "        ref = dict(np.load(f))\n",
        "\n",
        "    mu, sigma = calculate_inception_stats(image_path=image_path, num_expected=num_expected, max_batch_size=batch)\n",
        "    fid = calculate_fid_from_inception_stats(mu, sigma, ref['mu'], ref['sigma'])\n",
        "    return fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150cb0bc-343f-4650-8a80-48a5ea9f3338",
      "metadata": {
        "id": "150cb0bc-343f-4650-8a80-48a5ea9f3338"
      },
      "outputs": [],
      "source": [
        "for cfg in [0.0, 0.5, 2.0]:\n",
        "    sampling_params = {\n",
        "        'device': 'cuda',\n",
        "        'sigma_min': 0.02,\n",
        "        'sigma_max': 80.0,\n",
        "        'num_steps': 50,\n",
        "        'rho': 7.0,\n",
        "        'vis_steps': 1,\n",
        "        'stochastic': False,\n",
        "        'cfg': cfg,\n",
        "        'iter_start': 0,\n",
        "        'iter_end': 50,\n",
        "    }\n",
        "    save_model_samples('cunet_cond_samples', cond_model, sampling_params, batch_size=128, num_samples=3000, labels_usage='uniform')\n",
        "    fid = calc_fid('cunet_cond_samples', 'cmnist_train.npz', num_expected=3000, batch=128)\n",
        "    print('Модель с cfg = %.2g имеет FID = %.4g' % (sampling_params['cfg'], fid,))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "673cf883-5037-48a6-b6fe-821db031a5fc",
      "metadata": {
        "id": "673cf883-5037-48a6-b6fe-821db031a5fc"
      },
      "source": [
        "Как видим, эффект от CFG примерно следующий: при его добавлении с небольшим коэффициентом FID улучшается (за счет того, что разнообразие не сильно портится, а качество семплов становится лучше). Добавление же CFG с коэффициентом 2.0 качество ухудшает, поскольку слишком сильно портит разнообразие. На самом же деле, оба варианта использования CFG (с маленьким и большим коэффициентом) неоптимальны. Оптимальным будет использование CFG не на всем временном промежутке, а только на некотором его подотрезке. Таким образом, мы не только сможем ускорить генерацию из модели по сравнению с использованием CFG на всем промежутке, но и сможем добиться улучшением качества"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c991803-7111-4d36-a787-9ae163f366fd",
      "metadata": {
        "id": "5c991803-7111-4d36-a787-9ae163f366fd"
      },
      "source": [
        "## Задача 1 (0.5 баллов)\n",
        "\n",
        "Исследуйте влияние ширины и середины \"окна\" использования CFG на качество генерации. Зафиксируйте набор гиперпараметров в следующей ячейке, стартовый шум размерности $(10, 3, 32, 32)$, из которого будут сгенерированы 10 изображений, соответствующих 10 цифрам. Визуализируйте, как меняются картинки, сгенерированные из одного и того же шума, при сдвиге окна слева направо и при расширении ширины окна.\n",
        "\n",
        "Эксперимент со сдвигом окна слева направо лучше проводить со средней шириной окна (10-20 на 50 шагов генерации). После визуализации изменения качества при сдвиге слева направо, зафиксируйте позицию середины окна, которая, как вам кажется, порождает наиболее качественные семплы. После этого из полученной середины окна визуализируйте результаты генерации с различной шириной окна. Для обоих экспериментов лучше делать небольшой шаг по позиции или ширине (например, 5), чтобы не упустить резких качественных изменений.\n",
        "\n",
        "Опишите ваши наблюдения и выводы. В каком участке семплирования стоит использовать CFG, а где это не оправдано? Всегда ли увеличение ширины окна приводит к улучшению качества? Каким (приблизительно) значениям в терминах уровня шума $\\sigma$ соответствуют итерации, в которых, как вам кажется, имеет смысл использовать CFG?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb890e52-45d9-4817-bd5b-960c0f0c0e65",
      "metadata": {
        "id": "bb890e52-45d9-4817-bd5b-960c0f0c0e65"
      },
      "outputs": [],
      "source": [
        "# для визуализаций имеет смысл посмотреть код из 6 лекции\n",
        "# может пригодиться функция make_grid, которая принимает на вход батч изображений и визуализирует его\n",
        "\n",
        "sampling_params = {\n",
        "    'device': 'cuda',\n",
        "    'sigma_min': 0.02,\n",
        "    'sigma_max': 80.0,\n",
        "    'num_steps': 50,\n",
        "    'rho': 7.0,\n",
        "    'vis_steps': 1,\n",
        "    'stochastic': False,\n",
        "    'cfg': 2.0,\n",
        "    'iter_start': ...,\n",
        "    'iter_end': ...,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "830c7f2d-fed4-4eb4-ab2a-ad9a851ef1ea",
      "metadata": {
        "id": "830c7f2d-fed4-4eb4-ab2a-ad9a851ef1ea"
      },
      "source": [
        "## Задача 2\n",
        "Используя те же самые гиперпараметры семплинга (sigma_min=0.02, sigma_max=80.0, num_steps=50, rho=7.0, stochastic=False):\n",
        "Найдите такие гиперпарамеры CFG, чтобы FID полученных изображений (по 3000 картинок) был меньше, чем 2.8;\n",
        "* **(0.2 балла)** при условии, что использовано не более 25 шагов CFG;\n",
        "* **(0.1 балла)** При условии, что использовано не более 10 шагов CFG (подумайте, какой коэффициент CFG имеет смысл использовать в условии ограниченного бюджета);\n",
        "* Подробно распишите ваш процесс поиска оптимальных гиперпараметров: какой логикой вы руководствовались, как осуществляли перебор, как в итоге нашли полученные гиперпараметры. Два первых пункта без этого не принимаются (угадать нужные гиперпараметры практически невозможно, а описание процедуры в какой-то мере гарантирует самостоятельность результатов).\n",
        "* **(0.2 балла)** Сохраните все тройки вида (середина окна, ширина окна, FID), полученные в результате запусков ваших экспериментов. Если в первых пунктах получилось быстро угадать оптимальные параметры, запустите небольшой грид с перебором (строгих ограничений нет, но хотя бы 4х4 набора адекватных значений будет неплохо для визуализации). Попробуйте визуализировать собранную статистику и проанализировать ее, сопоставив с наблюдениями из первой задачи. Полезным здесь может быть визуализация результатов в виде двумерной heatmap (например, с помощью функции *pcolormesh* из *Matplotlib*) или иллюстрация графика зависимости FID от ширины окна при разных фиксированных серединах окна, или, если было перебрано небольшое количество гиперпараметров, scatter plot с осями (середина окна, ширина окна) и цветом, визуализирующем значение FID."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}